\documentclass{cshonours}
\usepackage{url}
\usepackage{graphics}
\usepackage{bibunits}
\usepackage{abbrevs}
\usepackage{acronym}
\usepackage[vario]{fancyref}
%\usepackage{gnuplot-lua-tikz}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{multicol}

%%% BEGIN LATEX TWEAKS

% Hyperref setup
\hypersetup{hidelinks}

% Configure bibliography
\bibliographystyle{acm}
\defaultbibliography{../references/primary}
\defaultbibliographystyle{acm}

% Acronyms for common stuff
\newcommand{\acrodefn}[3]{%
	\acrodef{#1}[#2]{#3}%
	\expandafter\newcommand\csname#1\endcsname{\ac{#1}\xspace}%
}
\acrodefn{aecl}{AECL}{Atomic Energy Canada Limited}
\acrodefn{cgr}{CGR}{Compagnie General Radiographique}
\acrodefn{fda}{FDA}{U.S. Food and Drug Administration}
\acrodefn{maude}{MAUDE}{Manufacturer and User Facility Device Experience Database}
\acrodefn{soup}{SOUP}{Software of Unknown Pedigree}
\acrodefn{iso}{ISO}{International Standards Organisation}
\acrodefn{iec}{IEC}{International Electrotechnical Commission}
\acrodefn{aami}{AAMI}{Association for the Advancement of Medical Instrumentation}

% Abbreviation commands for common stuff
% TODO: Fix spacing problem
\newabbrev\ther{Therac-25}
\newabbrev\etal{et al.}

% Fancyref support for subsections, source; https://github.com/openlilylib/tutorials/blob/master/aGervasoni/orchestralScores/example-materials/OLLbase.sty
\newcommand*{\fancyrefsubseclabelprefix}{subsec}

\fancyrefaddcaptions{english}{%
  \providecommand*{\frefsubsecname}{subsection}%
  \providecommand*{\Frefsubsecname}{Subsection}%
}

\frefformat{plain}{\fancyrefsubseclabelprefix}{\frefsubsecname\fancyrefdefaultspacing#1}
\Frefformat{plain}{\fancyrefsubseclabelprefix}{\Frefsubsecname\fancyrefdefaultspacing#1}

\frefformat{vario}{\fancyrefsubseclabelprefix}{%
  \frefsubsecname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyrefsubseclabelprefix}{%
  \Frefsubsecname\fancyrefdefaultspacing#1#3%
}


%%% END LATEX TWEAKS


\title{Therac-25:\\Will history repeat itself?}
\author{Ash Tyndall}

\begin{document}
\maketitle

\tableofcontents

\chapter{Introduction}

In the early 1970s, two companies, \aecl and \cgr, collaborated to build updated models of their core radiotheraputic offerings; Medical Linear Accelerators (LINACs). LINACs are an extension of the basic concept of a linear particle accelerator, repackaged for medical applications. They are are designed to create a beam of either electron or x-rays which can be focused on a very specific section of a patients body. These beams can be used to kill cancerous growths without severely damaging surrounding tissue.

Together, \aecl and \cgr develop two LINACs; the Therac-6 and the Therac-20, both of which were based on previous \cgr work, but with mechanical control substituted with control via computer terminal. These machines are designed with accelerators that could produce 6 MeV x-rays and 20 MeV x-rays/electrons respectively. Still in partnership in the mid-1970s, \aecl and \cgr develop a new Therac that uses a new kind of linear accelerator, a ``double-pass'' system, which reduces the space requirements and allows cheaper parts to be used. \aecl and \cgr part ways soon thereafter, citing competitive pressures. 

\aecl proceeded with the development of this new ``double-pass'' system, the \ther (so named for its 25 MeV x-ray and electron beam), continuing to develop the software-based control system, and passing the necessary regulatory requirements. The system is announced for public sale in 1983. One of the cutting-edge features of the machine is the removal of hardware-based interlocks and safeguards on the device, \aecl instead opting to use a wholly software-based approach to ensure that the appropriate components are rotated in front of the raw electron beam to reduce the dangerous radiation to therapeutic levels.

In 1985, the first reported case of \ther software safeguard failure occurred. Katherine Yarbrough, a breast cancer patient, receives an estimated 15,000--20,000 rads instead of the normal 200 rad range. She suffers severe radiation burns and shoulder and arm paralysis. A month later at a different facility, an unidentified female patient receives four separate overdoses totalling 13,000--17,000 rads within the space of several minutes, due to a combination of safeguard failure and poorly explained error messages. This patient dies of radiation induced cancer some months later.

It was initially unclear to those who operated the \ther that software errors were the cause of these overdoses. Due to the nature of radiation overdoses, the most serious of symptoms appear days if not weeks later, causing the resulting deaths and disablements to be attributed to other factors. However, over time, it became clear to different system operators that something was seriously wrong with the \ther, sparking an eventual forced FDA recall of the product with a total of six overdoses and two deaths.

% TODO: Cite
\ther was responsible for the first known deaths in radiotherapy, a profession that began some 35 years prior, and the confluence of death and computing caused an uproar at the time. Since then, the flames of anger have died down, and there is an opportunity to review the incident objectively. This report will examine the \ther case in detail, trying to determine the answers to several important questions:
\begin{enumerate}
 \item How does one design safe software, what does it involve, what are the pitfalls and how do we avoid them? (\Fref{chap:safesoftware})
 \item What were the flaws of \ther and how did failures on the part of \aecl and \cgr contribute to the creation of these flaws? (\Fref{chap:flawsfailures})
 \item How did standards and regulatory bodies respond to \ther through new guidelines and processes for creation of ``safe'' medical device software? (\Fref{chap:newstandards})
 \item Have those guidelines and processes resulted in the creation of safer medical device software? (\Fref{chap:data}) 
 \item Are there still areas in the medical landscape where regulation is insufficient? (\Fref{chap:reggaps})
 \item Will a disaster like \ther happen again? (\Fref{chap:conclusion})
\end{enumerate}

% TODO: Economic consequences of human death

\chapter{Literature Review}
\label{chap:litreview}
\ther was one of the first widely reported software-related disasters, and as a result, a variety of academic work has been performed since the disaster investigating the specifics of the failure of \ther, as well as the development of safety critical  medical software generally.

Primary work in the area of safety critical software development and system design as been done by Nancy Leveson. Her two books \textit{Safeware} \cite{safeware} and the more recent \textit{Engineering a Safer World} \cite{saferworld} are highly influential works in the field. They discuss from a variety of disciplines the causes of safety-critical system failure, from poor software testing processes to a failure of system design to adequately inform the user of the system's state. Of particular relevance to our \ther questions is \textit{Safeware} chapter 6, ``The Role of Humans in Automated Systems'', which describes several models of human-computer interaction and the necessary design principles to properly enable them. Additionally, \textit{Engineering a Safer World} chapter 9, ``Safety-Guided Design'', which discusses several of the \ther design flaws.

Leveson and Turner have also contributed the primary academic report on \ther \cite{leveson1993investigation} which discusses the history of the Therac brand, the history of the companies involved, as well as the timeline of the disasters that ensued.

Various work has been done into the area of safe critical software from both the perspective of cause and prevention. Dunn \cite{dunn2003designing} provides us with a helpful definition of safety in terms of ``mishap risk'', as well as examples of mishap causes.

In terms of cause, in \textit{Failure in Safety-Critical Systems} \cite[ch.~3]{johnson2003failure} Johnson discusses in detail the sources of failure, touching upon a broad variety of failures including Regulatory, Managerial, Hardware, Software, Human and Team based failures. Besnard and Baxter \etal \cite{besnard2003human} discuss two models of system failure, Reason's swiss cheese model and Randell's fault-error-failure model, both of which can be used to analyse the \ther disaster.

In terms of prevention, Nolan \cite{nolan2000system} proposes several strategies to be considered when designing ``safe systems of care'', as well as methods to reduce ``adverse events''  which may compromise patient health. Obradovich and Woods \cite{obradovich1996users} perform an investigation of poor Human-Computer Interface design in a medical device, describing how the device is flawed and how both the user and medical supervisor can change their processes to cope with this. Lin, Vicente and Doyle \cite{lin2001patient} propose a new interface for a specific medical appliance that applies human factors engineering, a key part of the discussed prevention of user error, and provides data demonstrating the effectiveness of such an approach from error minimisation and efficiency perspectives.

Several works discuss the introduction of various international standards and regulations post-\ther. Rakitin \cite{rakitin2006coping} provides an overview of foundational standards in the risk management space of medical device software. Brown \cite{brown2000overview} provides an overview of specifically the IEC 61508 ``Design of electrical / electronic / programmable electronic safety-related systems'' standard. Jordan \cite{jordan2006standard} provides an overview of specifically the IEC 62304 ``Medical Device Software -- Software Lifecycle Processes'' standard.

To allow us to determine if regulation of medical device software has improved the situation, several works which analyse relevant data will be examined. Wallace and Kuhn produced two papers \cite{wallace1999lessons,wallace2001failure} analysing a subset of \fda data relating to ``adverse events'' and provided statistics on the types of software errors and the medical domain of the devices, as well as information on how to prevent and detect these types of errors. The \fda \maude dataset \cite{maude} is also examined directly by the author to attempt to derive conclusions regarding the proportion of medical device software errors in the broader \maude database. Finally, \textit{Failure in Safety-Critical Systems} \cite[ch.~5]{johnson2003failure} discusses the under-reporting of incidents, as well as reporting bias.

\chapter{Safe software design:\\What does it involve?}
\label{chap:safesoftware}
The design of safe software (or as Leveson calls it, ``Safeware'' \cite{safeware}) is of paramount importance in the medical industry, as the software is frequently placed in direct control of equipment that has the potential to serious injure or kill if it operates outside (usually thin) safety margins. In the case of \ther, the machine was capable in certain failure states of emitting radiation that was able to cause death and permanent disablement.

It is difficult however to define what exactly ``safety'' is; granted, we can say that a system is ``safe'' if there is absolutely no chance that anything could go wrong with it, however, such systems do not exist in the real world. Dunn \cite{dunn2003designing} touches upon a useful concept to consider safety; a safe system is one with an acceptably low ``mishap risk''. Dunn uses the US Department of Defence definition of mishap; ``an unplanned event or series of events that result in death, injury, occupational illness, damage to or loss of equipment or property, or damage to the environment.'', suggesting that mishap risk deals with the impact or severity of a given mishap event, and the probability of it occurring. Another common team in the medical field that could be considered a synonym with mishap is ``adverse event'', which is more specifically and generally defined as ``any undesirable experience associated with the use of a medical product in a patient.'' \cite{fdaadverse}

Since \ther, much discussion has taken place on strategies to mitigate ``adverse events'' involving medical device software. Leveson argues that safe software design can only be achieved through creation of both a culture of safety, \cite[ch.~4]{safeware} and embedding Safeware design principles into the software from the beginning \cite[ch.~16]{safeware}. She argues that it is not enough to identify how a system can go wrong, but also institute changes that make those occurrence less likely by design. A strong culture of safety also ensures that complacency regarding safety, safety-apathetic organisational structures and lack of technical skill in implementing and ensuring safety in product are not present, which Leveson argues are key causes of software disaster.

Safeware design principles concern themselves primarily with hazard reduction as a primary goal and consideration when designing software. Leveson notes that in many industries outside of software, there exists a large body of knowledge; standards, codes of practice and checklists, which provide a large degree of guidance to the design of physical products with safety consideration. However, such codes are present to a lesser extent in software, and mainly concern themselves with concepts of readability and complexity, rather than ``safe'' architecture.

Within software, Leveson proposes four ways to reduce accidents through design changes. The first of these is hazard elimination, involving preventing hazards through design by making them physically impossible. Some possible hazard elimination methods include;
 \begin{itemize}
  \item \textbf{Substitution}; change components with hazardous effects with others
  \item \textbf{Simplification}; simply design such that hazards caused by complex interactions are removed
  \item \textbf{Decoupling}; reduce adverse effects of unexpected behaviour in a subsystem by making it only loosely coupled with other systems
  \item \textbf{Elimination of specific human error}; reduce obvious error by making system status unambiguous and easy to understand
  \item \textbf{Reduction of hazardous materials or conditions}; ensure software only contains code necessary for core functionality, remove unused code
 \end{itemize}
 
The second of these is hazard occurrence reduction, whereby the probability of a hazard occurring is reduced, such as by;
 \begin{itemize}
  \item \textbf{Design for controllability}; provide feedback about system state while critical actions occur, provide operator time to react to perceived problems with system
  \item \textbf{Barriers}; erect physical or logical barriers between safe and unsafe tasks, unsafe tasks should not be easy to do accidentally
  \item \textbf{Failure minimisation}; ensure safety factors and margins, as well as redundancy
 \end{itemize}
 
The third of these is hazard control, through which the probability of a hazard leading to an accident can be reduced through a variety of methods;
 \begin{itemize}
  \item \textbf{Limiting exposure}; critical software checks should be as close in execution as the code they protect, critical checks should also not be complimentary
  \item \textbf{Isolation and containment}; the number of people who could be affected by a hazard should be reduced to as low as practical
  \item \textbf{Protection systems and Fail-safe design}; independent watchdog hardware can be introduced to ensure that a failed system is returned back to a safe state
 \end{itemize}

The final of these is damage reduction. Potential hazards need to be understood, and emergency plans and training to handle these hazards should they arise should be provided to relevant people such than in the case of an hazard-caused accident, damage mitigating action can occur.

Leveson's proposed accident reduction techniques can be though of in terms of Besnard and Baxter's \cite{besnard2003human} integration of Reason and Randell's swiss-cheese and fault-error-failure models, shown in \Fref{fig:besnardmodel}. In this diagram we can see that the first three of Leveson's proposed accident reduction techniques serve as barriers through which adverse events cannot traverse, but though the introduction of faults in these layers, errors are able to arise that eventually become failures that cause holes in these barriers. If these barriers happen to be broken in the same area, it can lead to accidents, harm and loss of life. Safe software design involves instituting processes that ensure that as little faults are introduced into these barriers as possible.

\begin{figure}
 \includegraphics[width=\textwidth]{figs/besnardmodified.pdf}
 \caption{Model of Leveson's hazard reduction (adapted from \cite[fig.~6]{besnard2003human})}
 \label{fig:besnardmodel}
\end{figure}

Lin \cite{lin2001patient} provides a study of medical device interfaces which we can apply Leveson's proposed principles to investigate their effectiveness. In her study the user interface of a specific medical device, designed to administer specific quantities of a drug, was analysed against a proposed user interface which better applied Human-Computer Interface principles. This interface rearranged the design of the buttons used to input data, as well as changed the graphics displayed when navigating the device to give the user a much better sense of the implications of their actions, as well as the current mode of the machine. This could be considered a form of hazard elimination due to design; by reducing the confusion between the user's actions and the results, hazards related to erroneously entered parameters is greatly reduced. When statistically comparing the use of the old interface and the new interface, Lin noted an average reduction of error of 44\%.

Lin's changes to the interface were not drastic, but clearly demonstration how a safety orientated approach can be quite effective in preventing error.


\chapter{Flaws and failures:\\How and why?}
\label{chap:flawsfailures}
The \ther incidents are generally used as a case-study in literature on safety-critical devices, as they serve as very good example of a failure of software processes and ``safeware'' at multiple levels. As such, extensive investigation on the many facets of those failures exists, and a variety of research exists exploring the incident at multiple levels. Here we'll discuss some of the reasons \ther failed from a management and process perspective, as well as touching upon how some failures of user interface design exacerbated these errors.

The primary software error present in \ther was a race condition, triggered if the operator were to perform steps 3 and 4 of the following within 8 seconds;
\begin{enumerate}
 \item Press ``X'' to select X-ray in the Mode field.
 \item Press the ``Up'' arrow to re-position the cursor on the Mode field.
 \item Press ``E'' to change the selection to Electron mode.
 \item Press ``Enter'' to submit all other values unchanged and trigger the dispensing of radiation.
\end{enumerate}
This race condition existed due to a hard-coded timer 

The ultimate failure of \ther was the lack of safety as a primary consideration in the software development and design process. As described by Leveson and Turner in their investigation of the incident \cite{leveson1993investigation} little documentation was kept regarding the specifications of the software running \ther, nor on the testing process to ensure that the safety of the system was maintained. Software testing was only considered as part of broader whole-system tests, which would make complex software flaws much more difficult to find and fix. Furthermore, the software was developed by only one individual, leading to important questions about the amount of oversight that existed over the software quality and design. It is evident that \aecl's management either did not have the will or the competence to ensure the thorough testing of the \ther system, and a emphasis safety goals generally, with studies indicating that such an emphasis is one of the strongest indicators of a safe system. \cite[p.~415]{saferworld}

One of the serous process issues uncovered as part of the \ther investigation was the fact the software used in \ther was partially ported from the prior Therac-6, which while architecturally similar, contained electromechanical safeguards in addition to software based ones. With \aecl's track record with documentation in serious question, the Therac-6 software could rightly be considered \soup, as without extensive specific software testing having been performed on the Therac-6, one could not rightly assume that the Therac-6 software is free of bugs that could have adverse safety implications. It was later determined that one of the primary bugs responsible for the \ther deaths was also present in Therac-20, but due to the hardware safeguards, merely caused the machine to shut down, suggesting the bug may have partial genesis in this use of \soup.

Another serious issue with process was the near complete exclusion of software from core safety analysis steps of the \ther system, to quote \aecl's final report; 
\begin{quote}
Programming errors have been reduced by extensive testing on a hardware simulator and under field conditions on teletherapy units. Any residual software errors are not included in the analysis. \cite[p.~4]{leveson1993investigation}
\end{quote}
In that same report, \aecl assigns extraordinarily low probabilities to errors such as ``Computer selects wrong mode'' ($4 \times 10^{-9}$). This betrays \aecl's attitudes towards software safety, and their failure to consider the dangers of using \soup within their products. 

It is also illuminating to analyse the \ther disaster from the perspective of software design as a whole. Leveson \cite{safeware} provides several useful frameworks through which to analyse both a user and a computer's role within a given system. These are X, Y, Z (TODO: FILL IN). The most applicable of these to \ther would be human as monitor; a system in which the computer itself is responsible for the executive actions in the system, and the human serves a supervisory role, ensuring that the process is error free. This may seem at odds when considering the entire intelligence of the system being still vested in the human, however the actual execution of the actions are the sole domain of the computer system. Leveson goes on to discuss how in such a system, it is important that for the human to be sufficiently aware of the system's state to be an effective monitor; ``[t]he operator is dependent on the information provided; it is easy to provide too much or too little information or to display it poorly,.'' \cite[p.~TODO]{safeware} \ther failed to provide appropriate levels of information in several ways;
\begin{enumerate}
  \item Error messages were vauge with description, with the details contained within unexplained numerical codes, making it impossible for the operator to know specifically what was wrong. E.g. ``Error 1''.
  \item Both dangerous and benign failures were represented by the same error code, habituating operators to ignoring them.
\end{enumerate}

\ther is also a useful case study in counter-productive Human-Computer Interface (HCI) design. The system, for reasons of safety, required that parameters be entered multiple times, and if these entries mismatched, then an error would display and the treatment would not proceed. However, this ended with complaints from operators for arduousness, thus it was augmented with a system whereby the user could press Enter to confirm that a parameter set was correct. While efficient, this had the net result of habitualising the operators to press Enter in quick succession, thus reducing the safety gained from requiring careful consideration of the parameters. \cite[p.~274]{saferworld}

A similar HCI issue involved the ``Treatment Pause'' functionality, which allowed an operator to continue with treatment of a patient after receiving an error message up to five times before necessitating a system reboot. Due to the aforementioned poor error messages, this simply lead to the habitualisation of operators resuming treatment in the case of error, as in the vast majority of circumstances, this would have no ill effect. \cite[p.~301]{saferworld}

Ultimately it is difficult to pin down who at \aecl was responsible for the lack of appropriate safety culture, as subsequent lawsuits and regulatory investigations disincentivise parties with information from speaking out. However, due to the scale of the incident and the public nature of those lawsuits that did occur, enough information does exist to point to broad causes for the failure.


\chapter{Regulatory response:\\New standards}
\label{chap:newstandards}
In September 1987, two years after the first recorded \ther radiation overdose, the \fda announced that they intended to introduce regulations to prevent the failures of software quality management that had been present in \aecl. \cite{jacky1989programmed} The following two decades saw the introduction of a variety of international standards to deal with the various facets of medical device software safety. These regulations are not necessarily easily unified, to quote one attempt at doing so; ``with so many different standards, regulatory guidance papers and industry guides on RM [risk management], the task of collating this information into a usable model is itself daunting.'' \cite{burton2006risk} To reasonably restrict scope, only prominent United State national and international regulation from standards bodies regarding safe software development will be covered here.

Before the introduction of the first international standards, it was the sole job of the \fda to regulate medical device hardware within America. Information from this period is difficult to find, however, Leveson does mention the introduction of mandatory reporting by health-care facilities of adverse events in 1990. \cite[p.~TODO]{safeware}  This added to the preexisting requirements of manufacturers and importers to report such events.

One of the first international standards in this area that could potentially apply to medical device software is \textit{IEC 61508: Functional Safety of Electrical/Electronic/Programmable Electronic Safety-related Systems}, the first portions of which were published by the \iec in 1998. This standard provides a general set of standards creating a safety life cycle that can be adapted for specific industries. Notably, unlike \ther, it recognised that software risk cannot be reduced to zero, it can only be reduced, and as Leveson suggests, safety considerations must be present at every stage of the development lifecycle. \cite{iec61508} Jordan suggests however that the application of this standard to the medical area is difficult, as the area inherently has high risk, and the standard is most effective when dealing with low risk environments. \cite{jordan2006standard} 

In the same year, the \iso released their own standard specifically relating to medical device risk management; \textit{ISO 14971: Medical devices -- Application of risk management to medical device}. One of the key features of this standard was the requirement of evidence to exist in documentation that mitigation of risk in software was present, one of the key failings of \aecl. \cite{rakitin2006coping}

In 2000, a revision to parts 1--4 of \textit{IEC 60601: Medical Electrical Equipment} was released, which assisted in defining risk management more clearly, and adopted a definition of ``mishap risk'' that recognised both the severity and the probability of an issue. \cite{rakitin2006coping}

In 2001, the \aami in association with the American National Standards Institute released \textit{AAMI SW68: Medical device software - Software life cycle processes} which extended the risk management attributes ISO 14971 info a whole software development life cycle. \cite{rakitin2006coping} Burton, Mc Caffery and Richardson discuss how both ISO 14971 and AAMI SW68 were deficient in that they required risk management processes to be in place, but did not provide any specifics. \cite{burton2006risk}

In 2004, \textit{AAMI TIR32: Medical Device Software Risk Management} was published, which aimed to address the aforementioned deficiencies of ISO 14971 and AAMI SW68 by clarifying the risk management processes. \cite{rakitin2006coping} In particular, it provided clear definitions of reliability and safety, and importantly how they differ;
\begin{quote}
Reliability is the ability of a system to perform its required functions under stated conditions for a specified period of time. Safety is the probability that conditions (hazards) that can lead to a mishap do not occur, whether or not the intended function is performed. Reliability is interested in all possible software errors, while safety is concerned only with those errors that cause actual system hazards. \cite{aamitir32}
\end{quote}

Finally, in 2006, the \iec published the most recent standard in the area, \textit{IEC 62304: Medical device software -- Software life cycle processes}, which determines those processes, activities and tasks that are necessary for the creation of dependably and reliably medical device software. Huhn and Zechner note however that no specific models nor methods are prescribed to accomplish these tasks, the manufacturer must instead argue that their own processes allow for the creation of such dependable and reliable medical device software. \cite{huhn2010arguing} Jordan suggests that in the industry, such processes, activities and tasks are already core to reputable manufacturers, thus the lack of prescription is welcome to them. \cite{jordan2006standard}


\chapter{Data analysis:\\Are we safer?}
\label{chap:data}
While the discussion of regulation is useful, it is hard to say to what extent that has resolved the safety issues in medical device software that \ther demonstrated. Have regulators appropriately determined what is necessary to ensure these bugs are minimal? Is it possible to create software that has sufficiently low ``mishap risk'' in this realm?

\section{MAUDE}
There are several data sets that could potentially provide answers to these questions. The first of these is the \fda's \maude, a massive database of adverse events reported to the \fda available for download on their website. This dataset extends from the year 2000 to the current day.

\maude data consists of information relating to each adverse event including the manufacturer involved, the device involved, the category of adverse event and a free-form text field which contains further description from the manufacturer about the nature of the issue.

Using the \maude database, it may be possible to measure the proportion of issues manufacturers have reported as being related to medical device software in some form. From those statistics, it may be possible to derive trends, such as if device software is becoming more or less safe. The most useful data attribute for such a purpose is clearly the category: the \fda provides an extensive (nearly 1,000) list of categories, to one of which each adverse event can be assigned.

Because of the sheer size of \maude and time contraints, manual analysis of data is not fesiable, thus automated analysis techniques will be investigated. The first analysis on \maude involved determining a list of categories considered relevant to medical device software adverse events, and then querying the database to determine what proportion of adverse events each year were contained within those categories. The categories chosen for that purpose were;
\begin{multicols}{2}
{\smaller
\begin{itemize}
  \item Computer failure
  \item Computer hardware error
  \item Computer software issue
  \item Incorrect display
  \item Error or warning message, failure to produce
  \item Power calculation error due to software problem
  \item Incorrect software programming calculations
  \item Algorithms, inconsistent
  \item Semiautomatic code, failure to override
  \item Year 2000 (Y2K) related problem
  \item Date-related software issue
  \item Application network issue
  \item Application program issue
  \item Application program version or upgrade problem
  \item Application security issue
  \item Computer operating system issue
  \item Computer system security issue
  \item Data back-up problem
  \item Loss of Data
  \item Operating system becomes non-functional
  \item Operating system version or upgrade problem
  \item Problem with software installation
  \item Programming issue
\end{itemize}
}
\end{multicols}

\begin{figure}
  %\include{data/fdaplot1-report}
  \caption{Proportion of incidents in medical device software related categories over time}
  \label{fig:proportiongraph}
\end{figure}

\begin{figure}
  %\include{data/fdaplot2-report}
  \caption{Number of reports in \maude over time}
  \label{fig:totalgraph}
\end{figure}

The database was analysed, with the proportion of software events never becoming greater than 0.4\%. \Fref{fig:proportiongraph} shows the proportion of incidents of the above categories within the larger database. Upon analysing the data in this database, it is difficult to come to any complete conclusions about these proportions.

Before 2007, there are no adverse events corresponding to those categories in \maude, suggesting that either those categories were either non-existent, or the introduction of categories entirely was introduced in that year. From 2007--12, there are less than 10 reports matching the medical device software categories, suggesting that those categories are not readily used. 2013--14 shows significantly more of these categories, but they do not alone provide enough data points to draw any conclusions.

Wallace and Kuhn performed a previous study of this area with a manually classified non-public set of similar \fda data. They found that from 1983--91 approximately 6\% of all incidents could be considered to be software related, and this increasing in the years 1994--96, them having 11\%, 10\% and 9\% respectively. \cite{wallace2001failure} This is massively at odds with the data collected from the public \maude database, suggesting that the categorisation technique used is insufficient. Furthermore, it appears that the publicly accessible data within \maude is only a recently phenomenon, with \Fref{fig:totalgraph} showing how the public data present increases from nearly zero to over 30,000 in the period covered.

% TODO: Other data stuff
% TOOO: Mention trying to gt data off Kuhn


\chapter{Regulatory gaps:\\What's next?}
\label{chap:reggaps}
As discussed in the prior section, it is easily seen that regulation does not necessarily address problems with emerging technologies with appropriate speed. Crumpler and Rudolf \cite{crumpler1997fda} discuss a draft policy released by the \fda in 1989 designed to clarify and deal with the introduction of medical device software as components of sold products, or as accessories of those products, and how such software has the same regulations applied to it unless they are specifically and seperately classified. The \fda also defines accessories as software which mediates data input/output between the user and the medical device.

An interesting and relevant type of software in this area mentioned by Crumpter and Rudolf is radiation therapy treatment planning software, which allows medical professionals to import various scan data from a patient, allowing them to make a ``virtual patient'' thorough which they can view the patient's illness and construct a high level plan to treat that illness. This high level plan can then be deconstructed by the software into a series of less complex radiation exposure sizes and intensities. Such software has risen in prominence as computational power has increased to allow such complex to be fesiable.

Such software is of particular interest as in some cases such software is developed in-house, based on the specific needs of the clinic or medical centre. While under the 1989 policy the \fda would still require such software to be regulated, as it does meet the defnition of an accessory of a radiotherepy device, it becomes much more difficult for the \fda to do so effectively within it's framework. The \fda's main information gathering is contained within the ``pre-market notification'' requirements, in which software creators are required to notify the \fda before selling a product; this may never occur in the case of in-house solutions. Similarly, the \fda's main power exists in being able to take defective products ``off the market'' as it were, a measure that is not necessarily relevent or effective in the the case of in-house software.

It is difficult to measure how many accidents occur due to such software, however, some studies do exist on such accidents. Within the last decade France made changes to legal reporting obligations surrounding ionising radition that included medical applications. Derreumaux \etal \cite{derreumaux2008lessons} describe several accidents that were results of in-house medical device software error. 

In the first case, the software provided with the medical centre's LINAC was unable to handle situations involving a particular method of altering the beam. In-house software to simulate that circumstance was used, with the results of that simulation exported to the LINAC's Record \& Verify system. During the export process, the R\&V system was not properly configured to accept the data input format provided, thus setting a critical parameter in that mode to zero. This default value resulted in a radiation overdose of $>20\%$ over the course of the treatment, which lead to the patient's effected issue undergoing necrosis.

In the second less severe case, a new type of LINAC procedure was introduced which was encoded into a centre's in-house radiation therepy treatment planning software. It used a reference dose rate calculation that did not correctly take into account the differences in dosage caused by beam divergence in the new procedure. Due to these error, it is estimated hat around 4,000 people were exposed to an overdose ranging from 3\%--7\%.

Both of these cases were a result of software bugs that stringent testing procedures should have found and mitigated. To quote Derreumaux \etal;
\begin{quote}
The first accident throws light on the problem of in-house software: many radiotherapy centres use such handcrafted software, which are not standardised and may not be thoroughly checked before clinical use. \cite{derreumaux2008lessons}
\end{quote}

In this sense, it appears that in-house software may be the next ``wild-west'' of medical software, having far less regulation ensuring that appropriate software development and risk management practiceses are observed. 



\chapter{Conclusion:\\Will history repeat?}
\label{chap:conclusion}
The science of safe software has developed significantly since the occurrence of \ther, with a broad understanding now present as to the pitfalls of software development that can lead to safety critical software products that lack the proper emphasis on safety in their developement cycle. Leveson's \textit{Safeware} discusses in detail the various aspects of this, and here we touch upon those related to process that were particularly relevant in the case of \ther. We can view Leveson's proposed accident reduction techniques through the adaption of a unified failure model, as proposed by Besnard and Baxter.

The exact causes of \ther have been throughoughly studied by the safe software development community, with the root cause of \ther's most deadly bug being a race condition that triggered under very specific circumstances. Why that bug existed in the first place was a failure of \aecl to properly contemplate and integrate safe software creation principles into their development efforts. \aecl reused software from previous efforts, but did not adequately and seperately test the software in the changed circumstance of no hardware interlocks; they instead showed little regard for the role software bugs could play in dangerous situations in those submissions to the \fda that have been analysed. The user interface of \ther also assisted in habituating a variety of negative operator behaviours, such as disregarding error messages and continuing treatment.

Regulation in the medical device software area was partially driven to the forefront due to the publicised nature of the \ther incident, however despite this clear and present danger of software, national and international regulation has trickled in slowly over the subsequent decares. These standards are also complex, interwoven and difficult to easily follow, with a variety of standards bodies publishing different standards addressing different aspects of the process at different times. The primary and most recent standard appears to be \textit{IEC 62304: Medical device software -- Software life cycle processes} which was released for the first time in 2006.

Determining if these regulations have helped curb adverse events relating to software is a difficult task. One way to do this is to analyse the freely available data present in the \fda \maude database and consider the proportion of categorised data that was software related. While this avenue was initially promising, it appears that the dataset is poorly categorised for such a purpose, and manual categorisation would be outside of the time constraints of this report. The quantity of public data available each prior year also decreases significantly, making it difficult to draw statistically significant conclusions more than a few years in the past. Further study with a fully categorised and larger dataset would yeild more useful results.

One area that regulation may not adequately cover is that of in-house software accessories to LINACs and similar devices. While \fda recommendations as early as 1989 indicating that this software should be subject to regulation, the \fda is not well placed to perform such regulation, as the \fda's main methods of control relate to the restriction of sale of products, which is not relevant in the case of in-house software. Instances of in-house software bugs, potentially resulting from poorly regulated software developemnt, are discussed.

When analysing regulation pre-\ther and post-\ther, one can clearly say that the regulatory environment has improved, and that such regulation should help reduce those instances of adverse events relating to medical device software. However, the snails pace at which these regulations were introduced is very worrying, as the technological landscape can change far more quickly than regulators can respond. The rise of in-house software is one instance where regulation on this issue is not necessarily sufficient. 

\ther was an tragic disaster that helped highlight the necessity of safe software design processes. From \ther, many changes were instituted to try and prevent similar incidents from ocurring. Thus the question remains; will history repeat itself? From what we've seen here, it may already have.


\appendix

\bibliography{primary}

\end{document}