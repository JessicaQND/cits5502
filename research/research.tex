\documentclass{cshonours}
\usepackage{url}
\usepackage{graphicx}
\usepackage{bibunits}
\usepackage{abbrevs}
\usepackage{acronym}
\usepackage[vario]{fancyref}
\usepackage{gnuplot-lua-tikz}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{calc}
\usepackage{tikz}
\usepackage{ifthen}
\usepackage{listings}
\usepackage{geometry}
\input{chronology} % Something is wrong with the package; can't seem to properly install it

%%% BEGIN LATEX TWEAKS

% Hyperref setup
\hypersetup{hidelinks}

% Configure bibliography
\bibliographystyle{acm}
\defaultbibliography{../references/primary}
\defaultbibliographystyle{acm}

% Acronyms for common stuff
\newcommand{\acrodefn}[3]{%
	\acrodef{#1}[#2]{#3}%
	\expandafter\newcommand\csname#1\endcsname{\ac{#1}\xspace}%
}
\acrodefn{aecl}{AECL}{Atomic Energy Canada Limited}
\acrodefn{cgr}{CGR}{Compagnie General Radiographique}
\acrodefn{fda}{FDA}{U.S. Food and Drug Administration}
\acrodefn{maude}{MAUDE}{Manufacturer and User Facility Device Experience Database}
\acrodefn{soup}{SOUP}{Software of Unknown Pedigree}
\acrodefn{iso}{ISO}{International Standards Organisation}
\acrodefn{iec}{IEC}{International Electrotechnical Commission}
\acrodefn{aami}{AAMI}{Association for the Advancement of Medical Instrumentation}
\acrodefn{hci}{HCI}{human-computer interaction}

% Abbreviation commands for common stuff
% TODO: Fix spacing problem
\newcommand{\ther}{Therac-25\xspace}
\newcommand{\etal}{et al.\xspace}

% Fancyref support for subsections, source; https://github.com/openlilylib/tutorials/blob/master/aGervasoni/orchestralScores/example-materials/OLLbase.sty
\newcommand*{\fancyrefsubseclabelprefix}{subsec}

\fancyrefaddcaptions{english}{%
  \providecommand*{\frefsubsecname}{subsection}%
  \providecommand*{\Frefsubsecname}{Subsection}%
}

\frefformat{plain}{\fancyrefsubseclabelprefix}{\frefsubsecname\fancyrefdefaultspacing#1}
\Frefformat{plain}{\fancyrefsubseclabelprefix}{\Frefsubsecname\fancyrefdefaultspacing#1}

\frefformat{vario}{\fancyrefsubseclabelprefix}{%
  \frefsubsecname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyrefsubseclabelprefix}{%
  \Frefsubsecname\fancyrefdefaultspacing#1#3%
}


%%% END LATEX TWEAKS


\title{Therac-25:\\Will history repeat itself?}
\author{Ash Tyndall\\20915779}

\begin{document}
\maketitle

\tableofcontents

\chapter{Introduction}

In the early 1970s, two companies, \aecl and \cgr, collaborated to build updated models of their core radiotheraputic offerings; Medical Linear Accelerators (LINACs). LINACs are an extension of the basic concept of a linear particle accelerator, repackaged for medical applications. They are are designed to create a beam of either electron or x-rays which can be focused on a very specific section of a patients body. These beams can be used to kill cancerous growths without severely damaging surrounding tissue.

Together, \aecl and \cgr develop two LINACs; the Therac-6 and the Therac-20, both of which were based on previous \cgr work, but with mechanical control substituted with control via computer terminal. These machines are designed with accelerators that could produce 6 MeV x-rays and 20 MeV x-rays/electrons respectively. Still in partnership in the mid-1970s, \aecl and \cgr develop a new Therac that uses a new kind of linear accelerator, a ``double-pass'' system, which reduces the space requirements and allows cheaper parts to be used. \aecl and \cgr part ways soon thereafter, citing competitive pressures. 

\aecl proceeded with the development of this new ``double-pass'' system, the \ther (so named for its 25 MeV x-ray and electron beam), continuing to develop the software-based control system, and passing the necessary regulatory requirements. The system is announced for public sale in 1983. One of the cutting-edge features of the machine is the removal of hardware-based interlocks and safeguards on the device, \aecl instead opting to use a wholly software-based approach to ensure that the appropriate components are rotated in front of the raw electron beam to reduce the dangerous radiation to therapeutic levels.

In 1985, the first reported case of \ther software safeguard failure occurred. Katherine Yarbrough, a breast cancer patient, receives an estimated 15,000--20,000 rads instead of the normal 200 rad range. She suffers severe radiation burns and shoulder and arm paralysis. A month later at a different facility, an unidentified female patient receives four separate overdoses totalling 13,000--17,000 rads within the space of several minutes, due to a combination of safeguard failure and poorly explained error messages. This patient dies of radiation induced cancer some months later.

It was initially unclear to those who operated the \ther that software errors were the cause of these overdoses. Due to the nature of radiation overdoses, the most serious of symptoms appeared days if not weeks later, causing the resulting deaths and disablements to be attributed to other factors. However, over time, it became clear to different system operators that something was seriously wrong with the \ther, sparking an eventual forced \fda recall of the product with a total of six overdoses and two deaths.

% TODO: Cite
\ther was responsible for the first known deaths in radiotherapy, a profession that began some 35 years prior, and the confluence of death and computing caused an uproar at the time. Since then, the flames of anger have died down, and there is an opportunity to review the incident objectively. This report will examine the \ther case in detail, trying to determine the answers to several important questions:
\begin{enumerate}
 \item How does one design safe software, what does it involve, what are the pitfalls and how do we avoid them? (\Fref{chap:safesoftware})
 \item What were the flaws of \ther and how did failures on the part of \aecl and \cgr contribute to the creation of these flaws? (\Fref{chap:flawsfailures})
 \item How did standards and regulatory bodies respond to \ther through new guidelines and processes for creation of ``safe'' medical device software? (\Fref{chap:newstandards})
 \item Have those guidelines and processes resulted in the creation of safer medical device software? (\Fref{chap:data}) 
 \item Are there still areas in the medical landscape where regulation is insufficient? (\Fref{chap:reggaps})
 \item Will a disaster like \ther happen again? (\Fref{chap:conclusion})
\end{enumerate}

% TODO: Economic consequences of human death

\chapter{Literature Review}
\label{chap:litreview}
\ther was one of the first widely reported software-related disasters, and as a result, a variety of academic work has been performed since the disaster investigating the specifics of the failure of \ther, as well as the development of safety critical  medical software generally.

Primary work in the area of safety critical software development and system design as been done by Nancy Leveson. Her two books \textit{Safeware} \cite{safeware} and the more recent \textit{Engineering a Safer World} \cite{saferworld} are highly influential works in the field. They discuss from a variety of disciplines the causes of safety-critical system failure, from poor software testing processes to a failure of system design to adequately inform the user of the system's state. Of particular relevance to our \ther questions is \textit{Safeware} chapter 6, ``The Role of Humans in Automated Systems,'' which describes several models of \hci and the necessary design principles to properly enable them. Additionally, \textit{Engineering a Safer World} chapter 9, ``Safety-Guided Design,'' which discusses several of the \ther design flaws.

Leveson and Turner \cite{leveson1993investigation} have also contributed the primary academic report on \ther which discusses the history of the Therac brand, the history of the companies involved, as well as the timeline of the disasters that ensued.

Various work has been done into the area of safety critical software from both the perspective of cause and prevention. Dunn \cite{dunn2003designing} provides us with a helpful definition of safety in terms of ``mishap risk,'' as well as examples of mishap causes.

In terms of cause, in \textit{Failure in Safety-Critical Systems} \cite[ch.~3]{johnson2003failure} Johnson discusses in detail the sources of failure, touching upon a broad variety of failures including Regulatory, Managerial, Hardware, Software, Human and Team based failures. Besnard and Baxter \cite{besnard2003human} discuss two models of system failure, Reason's Swiss-cheese model and Randell's fault-error-failure model, both of which can be used to analyse the \ther disaster.

In terms of prevention, Nolan \cite{nolan2000system} proposes several strategies to be considered when designing ``safe systems of care,'' as well as methods to reduce ``adverse events''  which may compromise patient health. Obradovich and Woods \cite{obradovich1996users} perform an investigation of poor Human-Computer Interface design in a medical device, describing how the device is flawed and how both the user and medical supervisor can change their processes to cope with this. Lin, Vicente and Doyle \cite{lin2001patient} propose a new interface for a specific medical appliance that applies human factors engineering, a key part of the discussed prevention of user error, and provides data demonstrating the effectiveness of such an approach from error minimisation and efficiency perspectives.

Several works discuss the introduction of various international standards and regulations post-\ther. Rakitin \cite{rakitin2006coping} provides an overview of foundational standards in the risk management space of medical device software. Brown \cite{iec61508} provides an overview of specifically \textit{IEC 61508: Design of electrical / electronic / programmable electronic safety-related systems}. Jordan \cite{jordan2006standard} provides an overview of specifically \textit{IEC 62304: Medical Device Software -- Software Lifecycle Processes}.

To allow us to determine if regulation of medical device software has improved the situation, several works which analyse relevant data will be examined. Wallace and Kuhn produced two papers \cite{wallace1999lessons,wallace2001failure} analysing a subset of \fda data relating to ``adverse events'' and provided statistics on the types of software errors and the medical domain of the devices, as well as information on how to prevent and detect these types of errors. The \fda \maude dataset \cite{maude} is also examined directly by the author to attempt to derive conclusions regarding the proportion of medical device software errors in the broader \maude database. Finally, \textit{Failure in Safety-Critical Systems} \cite[ch.~5]{johnson2003failure} discusses the under-reporting of adverse events, as well as reporting bias.


\chapter{Safe software design: What and how?}
\label{chap:safesoftware}
The design of safe software (or as Leveson calls it, ``safeware'' \cite{safeware}) is of paramount importance in the medical industry, as in that field software is frequently placed in direct control of equipment that has the potential to cause serious injury or death if operated outside safety margins. In the case of \ther, the machine was capable in certain failure states of emitting radiation that was able to cause death and permanent disablement.

It is difficult however to define what exactly ``safety'' is. Granted, we can say that a system is ``safe'' if there is absolutely no chance that anything could go wrong with it, however, such systems do not exist in the real world. Dunn \cite{dunn2003designing} touches upon a useful concept to consider safety: A safe system is one with an acceptably low ``mishap risk.'' Dunn uses the US Department of Defence definition of mishap; ``an unplanned event or series of events that result in death, injury, occupational illness, damage to or loss of equipment or property, or damage to the environment,'' also suggesting that mishap risk deals with the impact or severity of a given mishap event, and the probability of it occurring. Another common team in the medical field that could be considered a synonym with mishap is ``adverse event'', which is more specifically and generally defined as ``any undesirable experience associated with the use of a medical product in a patient'' \cite{fdaadverse}.

Since \ther, there has been much discussion regarding strategies to mitigate ``adverse events'' involving medical device software. Leveson argues that safe software design can only be achieved through creation of both a culture of safety, \cite[ch.~4]{safeware} and embedding ``safeware'' design principles into the software from the beginning \cite[ch.~16]{safeware}. She argues that it is not enough to identify how a system can go wrong, but also institute changes that make those occurrence less likely by design. A strong culture of safety also ensures that complacency regarding safety, safety-apathetic organisational structures and lack of technical skill in implementing and ensuring safety in product are not present, which Leveson argues are key causes of software disaster.

``Safeware'' design principles concern themselves primarily with hazard reduction as a primary goal and consideration when designing software. Leveson notes that in many industries outside of software there exists a body of knowledge (i.e. standards, codes of practice and checklists) which provides a large degree of guidance to the design of physical products with safety considerations. However, such knowledge is present to a lesser extent in software, what knowledge that is present mainly concerns itself with concepts of readability and complexity, rather than ``safe'' architecture.

Within software, Leveson proposes four broad approaches to reduce accidents through design change. The first of these is hazard elimination, involving preventing hazards through design by making them physically impossible. Some possible hazard elimination methods include;
 \begin{itemize}
  \item \textbf{Substitution}: Change components with hazardous effects with others yhat lack those effects.
  \item \textbf{Simplification}: Simplify design such that hazards caused by complex interactions between components are removed.
  \item \textbf{Decoupling}: Reduce adverse effects of unexpected behaviour in a subsystem by making it only loosely coupled with other systems.
  \item \textbf{Elimination of specific human error}: Reduce obvious human error by making system status unambiguous and easy to understand.
  \item \textbf{Reduction of hazardous materials or conditions}: Ensure software only contains code necessary for core functionality, remove unused code.
 \end{itemize}
 
The second of these is hazard occurrence reduction, whereby the probability of a hazard occurring is reduced. Methods include;
 \begin{itemize}
  \item \textbf{Design for controllability}: Provide feedback about system state while critical actions occur, and provide operator time to react to perceived problems with system.
  \item \textbf{Barriers}: Erect physical or logical barriers between safe and unsafe tasks. Unsafe tasks should not be easy to do accidentally.
  \item \textbf{Failure minimisation}: Ensure safety factors and margins, as well as redundancy.
 \end{itemize}
 
The third of these is hazard control, through which the probability of a hazard leading to an accident can be reduced through a variety of methods;
 \begin{itemize}
  \item \textbf{Limiting exposure}: Critical software checks should be as close in execution as the code they protect, critical checks should also not be complimentary.
  \item \textbf{Isolation and containment}: The number of people who could be affected by a hazard should be reduced to as low as practical.
  \item \textbf{Protection systems and Fail-safe design}: Independent watchdog hardware can be introduced to ensure that a failed system is returned back to a safe state.
 \end{itemize}

The final of these is damage reduction. Potential hazards need to be understood, and emergency plans and training to handle these hazards should be provided. Relevant people can then ensure that in the case of an hazard-caused accident, damage mitigating action can occur.

Leveson's proposed accident reduction techniques can be though of in terms of Besnard and Baxter's \cite{besnard2003human} integration of Reason and Randell's Swiss-cheese and fault-error-failure models, shown in \Fref{fig:besnardmodel}. In this diagram we can see that the first three of Leveson's proposed accident reduction techniques serve as barriers through which adverse events cannot traverse. Though the introduction of faults in these layers, errors are able to arise that eventually become failures that cause holes in these barriers. If these barriers happen to be broken in the same area, it can lead to accidents, harm and loss of life. Safe software design involves instituting processes that ensure that as little faults are introduced into these barriers as possible.

\begin{figure}
 \includegraphics[width=\textwidth]{figs/besnardmodified.pdf}
 \caption{Model of Leveson's hazard reduction (adapted from \cite[fig.~6]{besnard2003human})}
 \label{fig:besnardmodel}
\end{figure}

Lin \cite{lin2001patient} provides a study of medical device interfaces which we can apply Leveson's proposed principles to investigate their effectiveness. In her study the user interface of a specific medical device, designed to administer specific quantities of a drug, was analysed against a proposed user interface which better applied Human-Computer Interface principles. This interface rearranged the design of the buttons used to input data, as well as changed the graphics displayed when navigating the device to give the user a much better sense of the implications of their actions, as well as the current mode of the machine.

This could be considered a form of hazard elimination due to design; by reducing the confusion between the user's actions and the results, hazards related to erroneously entered parameters can be greatly reduced. When statistically comparing the use of the old interface to the new, Lin noted an average user error reduction of 44\%. Lin's changes to the interface were not drastic, but clearly demonstration how a safety orientated approach can be quite effective in preventing error.


\chapter{Flaws and failures: How and why?}
\label{chap:flawsfailures}
The \ther incidents are generally used as a case-study in literature on safety-critical devices, as they serve as an illuminating example of a failure of software processes at multiple levels. As such, extensive investigation on the many facets of those failures exists, as well as a variety of research exploring the incident at multiple levels. Here we'll discuss some of the reasons \ther failed from a management and process perspective, as well as touching upon how some failures of user interface design exacerbated these errors.

\begin{figure}
 \centering
 \includegraphics[width=\textwidth]{figs/interface.pdf}
 \caption{\ther interface}
 \label{fig:theracui}
\end{figure}

The primary software error present in \ther was a race condition. In normal operation the program would wait 8 seconds when the beam mode was changed, as during that time physical elements of the machine were moving into place, leaving the beam improperly covered by either the X-ray or the electron beam dispersal elements for the duration of that time. Because of energy losses in the beam spreading apparatus, the raw power of the beam was set much higher in electron mode to have comparable effects to X-rays.  If the operator were to quickly change from X-ray to electron type the 8 second wait would not be enforced. If the beam was activated in less than 8 seconds after a type change, the apparatus would still be rotating, and proper implement would not be in place, thus severely irradiating the patient with the unintended full energy of the beam.

Specifically, the bug was triggered, thus causing an overdose, by the completion of steps 3--4 of the below in less than 8 seconds;
\begin{enumerate}
 \item Press ``X'' to select X-ray in the Beam Type field.
 \item Press the ``Up'' arrow to re-position the cursor on the Beam Type field.
 \item Press ``E'' to change the selection to Electron mode.
 \item Press ``Enter'' a number of times to submit all other values unchanged and trigger the dispensing of radiation.
\end{enumerate}

The ultimate failure of \ther was the lack of safety as a primary consideration in the software development and design process. As described by Leveson and Turner \cite{leveson1993investigation} in their investigation of the incident little documentation was kept regarding the specifications of the software running \ther, nor on the testing processes performed to ensure that the safety of the system was maintained. Software testing was only considered as part of broader whole-system tests, which would make complex software flaws much more difficult to find and fix. Furthermore, evidence indicates the software was developed by only one individual, leading to important questions regarding the amount of oversight that existed over the \ther software quality and design. It is evident that \aecl's management either did not posses the will and/or the competence to ensure the thorough testing of the \ther system, and a emphasis safety goals generally, with studies indicating that such an emphasis is one of the strongest indicators of a safe system \cite[p.~415]{saferworld}.

One of the serous process issues uncovered as part of the \ther investigation was the fact the software used in \ther was partially ported from the prior Therac-6, which while architecturally similar, crucially differed in that it contained electromechanical safeguards in addition to software based ones. With \aecl's track record with documentation in serious question, the Therac-6 software could rightly be considered \soup, as without extensive specific software testing having been performed on the Therac-6, one could not rightly assume that the Therac-6 software was free of bugs that could have had adverse safety implications. It was later determined that one of the primary bugs responsible for the \ther incident was also present in Therac-20, but due to the hardware safeguards it merely caused the machine to shut down, suggesting the bug may have partial genesis in this use of \soup.

Another serious issue with process was the near complete exclusion of software from the core safety analysis of the \ther system. To quote \aecl's final report; 
\begin{quote}
Programming errors have been reduced by extensive testing on a hardware simulator and under field conditions on teletherapy units. Any residual software errors are not included in the analysis \cite[p.~4]{leveson1993investigation}.
\end{quote}
In that same report, \aecl assigns extraordinarily low probabilities to errors such as ``Computer selects wrong mode'' ($4 \times 10^{-9}$). This betrays \aecl's attitude towards software safety, and their failure to consider the dangers of using \soup within their products. 

It is also illuminating to analyse the \ther disaster from the perspective of software design as a whole. Leveson \cite{safeware} provides several useful frameworks through which to analyse both a user and a computer's role within a given system. These are the human as monitor, backup and partner models. The most applicable of these to \ther would be ``human as monitor;'' a system in which the computer itself is responsible for the executive actions in the system, and the human serves a supervisory role ensuring that the process is error free. This may seem at odds when considering the entire ``directive'' intelligence of the system being still vested in the human, however the actual execution of the actions are the sole domain of the computer system. Leveson goes on to discuss how in such a system, it is important for the human to be sufficiently aware of the system's state to be an effective monitor; ``[t]he operator is dependent on the information provided; it is easy to provide too much or too little information or to display it poorly'' \cite[p.~114]{safeware}. \ther failed to provide appropriate levels of information to the human monitor in several ways;
\begin{enumerate}
  \item Error messages were vauge with description, with the details contained within unexplained numerical codes, making it impossible for the operator to know specifically what was wrong. E.g. ``HTILT 1''.
  \item Both dangerous and benign failures were represented by the same error code, habituating operators to ignore the messages.
\end{enumerate}

\ther is also a useful case study in poor \hci design. The system, for reasons of safety, required that parameters be entered multiple times, and if these entries mismatched, then an error would display and the treatment would not proceed. However, this ended with complaints from operators due to its arduous nature, thus it was augmented with a system whereby the user could press Enter to confirm that a parameter set was correct. While efficient, this had the net result of habitualising the operators into pressing Enter in quick succession, thus reducing the safety gained from requiring careful consideration of the parameters \cite[p.~274]{saferworld}.

A similar HCI issue involved the ``Treatment Pause'' functionality, which allowed an operator to continue with treatment of a patient after receiving an error message up to five times before necessitating a system reboot. Due to the aforementioned poor error messages, this simply lead to the habitualisation of operators resuming treatment in the case of any error, as in the vast majority of circumstances, this would have no ill effect \cite[p.~301]{saferworld}.

Ultimately it is difficult to pin down who at \aecl was responsible for the lack of appropriate safety culture, as subsequent lawsuits and regulatory investigations disincentivise parties with information from speaking out. However, due to the scale of the incident and the public nature of those lawsuits that did occur, enough information does exist to point to broad causes for the failure.


\chapter{Regulatory response: New standards}
\label{chap:newstandards}
In September 1987, two years after the first recorded \ther radiation overdose, the \fda announced that they intended to introduce new regulation to prevent the failures of software quality management that had been present in \aecl from reoccurring \cite{jacky1989programmed}. The following two decades saw the slow introduction of a variety of US national and international standards to deal with the various facets of medical device software safety. These regulations are not necessarily easily unified. To quote one attempt; ``with so many different standards, regulatory guidance papers and industry guides on RM [risk management], the task of collating this information into a usable model is itself daunting'' \cite{burton2006risk}. Regulation is highly complex, thus to reasonably restrict scope, only prominent and relevant United States national and international regulation from standards bodies and agencies will be covered.

Before the introduction of the first international standards, it was the sole job of the \fda to regulate medical device hardware within the US. Information from this period is difficult to find, however, Leveson does mention the introduction of mandatory reporting by health-care facilities of adverse events in 1990 \cite[p.~13]{leveson1993investigation}.  This added to the preexisting requirements of manufacturers and importers to report such events.

\begin{figure}
  \centering
  \begin{chronology}[5]{1984}{2006}{\textwidth}
   \event{1984}{First production \ther}
    \event[1985]{1988}{\ther incidents}
    \event{1990}{Mandatory reporting}
    \event{1998}{IEC 61508, ISO 14971}
    \event{2000}{IEC 60601}
    \event{2001}{AAMI SW68}
    \event{2004}{AAMI TIR32}
    \event{2006}{IEC 62304}
  \end{chronology}
  \caption{Regulation timeline}
  \label{fig:regtimeline}
\end{figure}


One of the first international standards that was discussed as a potential contender for the field is \textit{IEC 61508: Functional Safety of Electrical / Electronic / Programmable Electronic Safety-related Systems}, the first portions of which were published by the \iec in 1998. This standard provides a general set of standards creating a safety life-cycle that could be adapted for specific industries. Notably, unlike \aecl, it recognised that software risk cannot be reduced to zero, it can only be reduced, and as Leveson suggests, safety considerations must be present at every stage of the development lifecycle \cite{iec61508}. Jordan \cite{jordan2006standard} suggests however that the application of this standard to the medical area is difficult, as the area inherently has high risk, and the standard is most effective when dealing with low risk environments.

In the same year, the \iso released their own standard specifically relating to medical device risk management; \textit{ISO 14971: Medical devices -- Application of risk management to medical device}. One of the key features of this standard was the requirement of evidence to exist in documentation that mitigation of risk in software was present, one of the key failings of \aecl \cite{rakitin2006coping}.

In 2000, a revision to parts 1--4 of \textit{IEC 60601: Medical Electrical Equipment} was released, which assisted in defining risk management more clearly, and adopted a definition of ``mishap risk'' that recognised both the severity and the probability of an issue \cite{rakitin2006coping}.

In 2001, the \aami in association with the American National Standards Institute released \textit{AAMI SW68: Medical device software -- Software life cycle processes} which extended the risk management attributes of ISO 14971 into a whole software development life cycle \cite{rakitin2006coping}. Burton, Mc Caffery and Richardson \cite{burton2006risk} suggest that both ISO 14971 and AAMI SW68 were deficient in that they required risk management processes to be in place, but did not provide any specifics as to the form such a process would take.

In 2004, \textit{AAMI TIR32: Medical Device Software Risk Management} was published, which aimed to address the aforementioned deficiencies of ISO 14971 and AAMI SW68 by clarifying the risk management processes \cite{rakitin2006coping}. In particular, it provided clear definitions of reliability and safety, and importantly how they differ;
\begin{quote}
Reliability is the ability of a system to perform its required functions under stated conditions for a specified period of time. Safety is the probability that conditions (hazards) that can lead to a mishap do not occur, whether or not the intended function is performed. Reliability is interested in all possible software errors, while safety is concerned only with those errors that cause actual system hazards \cite{aamitir32}.
\end{quote}

Finally, in 2006 the \iec published the most recent standard in the area, \textit{IEC 62304: Medical device software -- Software life cycle processes}, which specifies those processes, activities and tasks that are necessary for the creation of dependable and reliable medical device software. Huhn and Zechner note however that no specific models nor methods are prescribed to accomplish these tasks, the manufacturer must instead argue that their own processes allow for the creation of such software that fits the standard's criteria \cite{huhn2010arguing}. Jordan suggests that in the industry, such processes, activities and tasks are already core to reputable manufacturers, thus the lack of prescription is welcome to them \cite{jordan2006standard}.


\chapter{Data analysis: Are we safer?}
\label{chap:data}
While the discussion of regulation is useful, it is hard to say to what extent that regulation has resolved the safety issues in medical device software that \ther demonstrated. Have regulators appropriately determined what is necessary to ensure these bugs are minimal? Is it possible to create software that has sufficiently low ``mishap risk'' in this realm?

One data set that could potentially provide answers to these questions is the \fda's \maude, a massive database of adverse events reported to the \fda available for download on their website. This dataset extends from the year 2000 to the current day.

\maude data consists of information relating to each adverse event, including the manufacturer involved, the device involved, the category of adverse event and a free-form text field which contains further description from the manufacturer about the nature and specifics of the adverse event.

Using \maude, it may be possible to measure the proportion of issues manufacturers have reported as being related to medical device software in some form. From those statistics, it may be possible to derive trends, such as if device software is becoming more or less safe over time. The most useful data attribute for such a purpose is clearly the category; the \fda provides an extensive (nearly 1,000) list of categories, to one of which each adverse event can be assigned.

Because of the sheer size of \maude and time constraints, manual analysis of data is not feasible, thus automated analysis techniques will be investigated. The analysis technique on \maude chosen involved determining a list of categories considered relevant to medical device software adverse events, and then querying the database to determine what proportion of adverse events each year were contained within those categories. The categories chosen for that purpose are shown in \Fref{tab:fdacategories}.

\begin{figure}
  \centering
  \include{data/fdaplot1-report}
  \caption{Proportion of incidents in ``software related'' categories over time}
  \label{fig:proportiongraph}
\end{figure}

\begin{figure}
  \centering
  \include{data/fdaplot2-report}
  \caption{Number of reports in \maude over time (log scale)}
 \label{fig:totalgraph}
\end{figure}

\begin{table}
\begin{multicols}{2}
{\smaller
\begin{itemize}
  \item Computer failure
  \item Computer hardware error
  \item Computer software issue
  \item Incorrect display
  \item Error or warning message, failure to produce
  \item Power calculation error due to software problem
  \item Incorrect software programming calculations
  \item Algorithms, inconsistent
  \item Semiautomatic code, failure to override
  \item Year 2000 (Y2K) related problem
  \item Date-related software issue
  \item Application network issue
  \item Application program issue
  \item Application program version or upgrade problem
  \item Application security issue
  \item Computer operating system issue
  \item Computer system security issue
  \item Data back-up problem
  \item Loss of Data
  \item Operating system becomes non-functional
  \item Operating system version or upgrade problem
  \item Problem with software installation
  \item Programming issue
\end{itemize}
}
\end{multicols}
  \caption{``Software related'' categories in \maude}
  \label{tab:fdacategories}
\end{table}

The database was analysed, with the proportion of software events never becoming greater than 0.4\%. \Fref{fig:proportiongraph} shows the proportion of incidents of the above categories within the larger database. Upon analysing the data in this database, it is difficult to come to justifiable complete conclusions about these proportions.

Before 2007 there are no adverse events corresponding to the aforementioned categories in \maude, suggesting that either those categories were either non-existent, or the introduction of categories entirely was introduced in that year. From 2007--12, there are less than 10 reports matching those medical device software categories, suggesting that those categories are not readily used. 2013--14 shows significantly more items in these categories, but those years do not alone provide enough data points to draw any conclusions.

Wallace and Kuhn \cite{wallace2001failure} performed a previous study of this area with a manually classified non-public set of similar \fda data. They found that from 1983--91 approximately 6\% of all incidents could be considered to be software related, with this increasing in the years 1994--96, to 11\%, 10\% and 9\% respectively. This is massively at odds with the data collected from the public \maude database, suggesting that the categorisation technique used is insufficient. Furthermore, it appears that the publicly accessible data records within \maude is only a recent phenomenon, with \Fref{fig:totalgraph} showing how the public data present per year increases from nearly zero to over 30,000 in the period covered.

% TODO: Other data stuff
% TOOO: Mention trying to gt data off Kuhn



\chapter{Regulatory gaps: What's next?}
\label{chap:reggaps}
As discussed in the prior section, it is easily seen that regulation does not address problems with emerging technologies with necessary speed. Crumpler and Rudolf \cite{crumpler1997fda} discuss a draft policy released by the \fda in 1989 designed to clarify and deal with the introduction of medical device software as components of sold products, or as accessories of those products: Such software would have the same regulations applied to it as a ``parent'' product unless it is specifically and separately classified. The \fda defines accessories as software which mediates data input/output between the user and the medical device.

An interesting and relevant type of software in this area is radiation therapy treatment planning software, which allows medical professionals to import various scan data from a patient, and constructs a ``virtual patient'' thorough which they can review the patient's situation and construct a high level plan to treat their illness. This high level plan can then be deconstructed by the software into a series of less complex radiation exposure specifics. Such software has risen in prominence as computational power has increased to allow such complex simulations to be feasible.

Such software is of particular interest as in some cases such software is developed in-house, so that it may handle the specific needs of a medical centre. While under the 1989 policy the \fda would still require such software to be regulated, as it does meet the definition of an accessory of a radiotherapy device, it becomes much more difficult for the \fda to do so effectively within its framework. The \fda's main information gathering is contained within the ``pre-market notification'' requirements, in which software creators are required to notify the \fda before selling a product; this may never occur in the case of in-house solutions, making it difficult for the \fda to know about such development in the first place. Similarly, the \fda's main power exists in being able to take defective products ``off the market'' as it were, a measure that is not necessarily relevant or effective in the the case of in-house software.

It is difficult to measure how many accidents occur due to in-house software, however, some studies do exist on such accidents. Within the last decade France made changes to legal reporting obligations surrounding ionising radiation that included medical applications, Derreumaux \etal \cite{derreumaux2008lessons} describe several accidents that were results of in-house radiation treatment planning medical device software error that can be analysed.

In the first case described, the software provided with the medical centre's LINAC was unable to handle situations involving a particular method of altering the beam. In-house software to simulate that circumstance was used, with the results of that simulation exported to the LINAC's Record \& Verify system. During the export process, the R\&V system was not properly configured to accept the data input format provided, thus misinterpreting the data and setting a critical parameter in that mode to zero. This default value resulted in a radiation overdose of $>20\%$ over the course of the treatment, which lead to the patient's effected issue undergoing necrosis.

In the second less severe case, a new type of LINAC procedure was introduced which was encoded into a centre's in-house radiation therapy treatment planning software. It used a reference dose rate calculation that did not correctly take into account the differences in dosage caused by beam divergence in the new procedure. Due to these error, it is estimated hat around 4,000 people were exposed to an overdose ranging from 3\%--7\%.

Both of these cases were a result of software bugs that stringent testing procedures should have found and mitigated. To quote Derreumaux \etal;
\begin{quote}
The first accident throws light on the problem of in-house software: many radiotherapy centres use such handcrafted software, which are not standardised and may not be thoroughly checked before clinical use \cite{derreumaux2008lessons}.
\end{quote}

In this sense, it appears that in-house software may be the next ``wild-west'' of medical software, having far less regulation ensuring that appropriate software development and risk management practices are observed. 



\chapter{Conclusion: Will history repeat?}
\label{chap:conclusion}
The science of safe software has developed significantly since the \ther incident, with a broad understanding now present as to the pitfalls of software development that can lead to safety critical software products that lack the proper emphasis on safety in their development cycle. Leveson's \textit{Safeware} discusses in detail the various aspects of this, and here we touch upon those related to process that were particularly relevant in the case of \ther. We can view Leveson's proposed accident reduction techniques through the useful adaptation of a unified failure model, as proposed by Besnard and Baxter.

The exact causes of \ther have been thoroughly studied by the safe software development community, with the root cause of \ther's most deadly bug being a race condition that triggered under very specific circumstances. Why that bug existed in the first place was a failure of \aecl to properly contemplate and integrate safe software creation principles into their development efforts. \aecl reused software from previous efforts, but did not adequately and separately test the software in the changed circumstance of no hardware interlocks; they instead showed little regard for the role software bugs could play in safety critical software. The user interface of \ther also failed by habituating a variety of negative operator behaviours, such as disregarding error messages and continuing treatment after receiving error messages.

Regulation in the medical device software area was partially driven to the forefront due to the publicised nature of the \ther incident, however despite this clear and present danger of software, national and international regulation has only trickled in slowly over the subsequent decades. These standards are also complex, interwoven and difficult to easily follow, with a variety of standards bodies publishing different standards addressing different aspects of the process at different times. The most recent standard in the area appears to be \textit{IEC 62304: Medical device software -- Software life cycle processes} which was released for the first time in 2006.

Determining if these regulations have helped curb adverse events relating to software is a difficult task. One way to do this is to analyse the freely available data present in the \fda \maude database and consider the proportion of categorised data that was software related. While this avenue was initially promising, it appears that the dataset is poorly categorised for such a purpose, and manual categorisation would be outside of the scope of this report. The quantity of public data available each prior year also decreases significantly, making it difficult to draw statistically significant conclusions more than a few years in the past. Further study with a fully categorised and larger dataset would yield more useful results.

One area that regulation may not adequately cover is that of in-house software accessories to LINACs and similar devices. While \fda recommendations as early as 1989 indicating that this software should be subject to regulation, the \fda is not well placed to perform such regulation, as the \fda's main methods of control relate to the restriction of sale of products, which is not relevant in the case of in-house software. Instances of in-house software bugs, potentially resulting from poorly regulated software development, are discussed, with overdoses similar to those caused by \ther resulting.

When analysing regulation pre and post-\ther, one can clearly say that the regulatory environment has improved, and that such regulation should help reduce those instances of adverse events relating to medical device software. However, the snails pace at which these regulations were introduced is very worrying, as the technological landscape can change far more quickly than regulators can respond. The rise of in-house software is one instance where regulation on this issue is not necessarily sufficient to ensure patient safety.

\ther was an tragic disaster that helped highlight the necessity of safe software design processes. From \ther, many changes were instituted to try and prevent similar incidents from occurring. Thus the question remains; will history repeat itself? From what we've seen here, it may already have.

\appendix

\chapter{MAUDE data analysis}

Data retrieved from \cite{maude} on 20 September 2014.

\section{Raw results}

\begin{tabular}{|l|r|r|r|}
\hline
Year & Software Reports & Total Reports & Percentage \\ \hline
1977\textsuperscript{1} & 0 & 1 & 0 \\
1988\textsuperscript{1} & 0 & 1 & 0 \\
1991\textsuperscript{1} & 0 & 1 & 0 \\
1995 & 0 & 2 & 0 \\
1996 & 0 & 4 & 0 \\
1998 & 0 & 4 & 0 \\
1999 & 0 & 1 & 0 \\
2000 & 0 & 6 & 0 \\
2001 & 0 & 16 & 0 \\
2002 & 0 & 22 & 0 \\
2003 & 0 & 142 & 0 \\
2004 & 0 & 216 & 0 \\
2005 & 0 & 311 & 0 \\
2006 & 0 & 495 & 0 \\
2007 & 1 & 577 & 0.173310225 \\
2008 & 1 & 805 & 0.124223602 \\
2009 & 4 & 1196 & 0.334448161 \\
2010 & 2 & 1442 & 0.138696255 \\
2011 & 8 & 3232 & 0.247524752 \\
2012 & 6 & 4239 & 0.141542817 \\
2013 & 55 & 16163 & 0.340283363 \\
2014 & 120 & 32621 & 0.367861194 \\ \hline
\end{tabular}

\textsuperscript{1} Excluded due to lack of continuity.

\clearpage
\section{Processing code}
\fontsize{10pt}{12pt}\selectfont
\begin{lstlisting}[language=Python]
# Written for Python 3.4
import csv, pprint, datetime, random

incidents = {}
incident_problem = {}
incident_text = {}
key_resolve = {}
code_problem = {}
problem_code = {}

computer_related = [
  'Computer failure',
  'Computer hardware error',
  'Computer software issue',
  'Incorrect display',
  'Error or warning message, failure to produce',
  'Power calculation error due to software problem',
  'Incorrect software programming calculations',
  'Algorithms, inconsistent',
  'Semiautomatic code, failure to override',
  'Year 2000 (Y2K) related problem',
  'Date-related software issue',
  'Application network issue',
  'Application program issue',
  'Application program version or upgrade problem',
  'Application security issue',
  'Computer operating system issue',
  'Computer system security issue',
  'Data back-up problem',
  'Loss of Data',
  'Operating system becomes non-functional',
  'Operating system version or upgrade problem',
  'Problem with software installation',
  'Programming issue',
]

date_resolve = [
  'DATE_OF_EVENT',
  'DATE_FACILITY_AWARE',
  'DATE_MANUFACTURER_RECEIVED',
  'DATE_REPORT_TO_MANUFACTURER',
  'DATE_REPORT_TO_FDA',
  'DATE_REPORT',
  'REPORT_DATE',
  'DATE_RECEIVED',
]

print('Reading in data...')
with open('deviceproblemcodes.txt', newline='') as csvfile:
  mdrreader = csv.reader(csvfile, delimiter='|', quoting=csv.QUOTE_NONE)
  
  for code, name in mdrreader:
    code_problem[int(code)] = name
    problem_code[name] = int(code)    

with open('foidevproblem.txt', newline='') as csvfile:
  mdrreader = csv.reader(csvfile, delimiter='|', quoting=csv.QUOTE_NONE)
  
  for report, code in mdrreader:
    if code.isdigit():
      incident_problem[int(report)] = int(code)    

with open('foitext.txt', newline='') as csvfile:
  mdrreader = csv.reader(csvfile, delimiter='|', quoting=csv.QUOTE_NONE)
  
  for report, _, _, _, _, text in mdrreader:
    if not (len(text) < 10 and ("(B)(4)" in text or "(B)(6)" in text)):
      incident_text[int(report)] = text

with open('mdrfoi.txt', newline='') as csvfile:
  mdrreader = csv.reader(csvfile, delimiter='|', quoting=csv.QUOTE_NONE)
  
  skip_first = True
  for row in mdrreader:
    if skip_first:
      key_resolve = {y : x for (x, y) in enumerate(row)}  
      skip_first = False
    else:
      report_key = int(row[key_resolve['MDR_REPORT_KEY']])
      adverse_event = row[key_resolve['ADVERSE_EVENT_FLAG']] == 'Y'
      product_problem = row[key_resolve['PRODUCT_PROBLEM_FLAG']] == 'Y'
      
      if report_key not in incident_problem:
        continue
        
      err_code = incident_problem[report_key]
      
      # The FDA data has dates of varying quality, find the best one
      year = 0
      for r in date_resolve:
        date_str = row[key_resolve[r]]
        if date_str == '':
          continue
        
        date = datetime.datetime.strptime(date_str, '%m/%d/%Y')
        year = date.year
        
        if year > datetime.datetime.now().year or year < 1965:
          continue
        
        break
      else:
        print('Could not determine date format!')
        raise Exception

      if year not in incidents:
        incidents[year] = {}
        
      if err_code not in incidents[year]:
        incidents[year][err_code] = []
        
      incidents[year][err_code].append(row)

sincidents = sorted(incidents.items())

comp_text = set()
non_comp_text = set()

print("Processing...")
with open('stats2.csv', 'w', newline='') as resfile:
  csvw = csv.writer(resfile)

  for year, edict in sincidents:
    comp_count = 0
    all_count = 0
    computer_related_ids = [problem_code[x] for x in computer_related]
    
    for ecode, records in edict.items():
      if ecode in computer_related_ids:
        comp_count += len(records)

      for r in records:
        key = int(r[key_resolve['MDR_REPORT_KEY']])
        if key in incident_text:
          if ecode in computer_related_ids:
            comp_text.add(incident_text[key])
          else:
            non_comp_text.add(incident_text[key])
        
      all_count += len(records)

    csvw.writerow([year, comp_count, all_count])
\end{lstlisting}

\bibliography{primary}

\end{document}